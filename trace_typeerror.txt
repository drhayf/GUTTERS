============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.2, pluggy-1.6.0 -- C:\dev\GUTTERS\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\dev\GUTTERS
configfile: pyproject.toml
plugins: anyio-4.12.1, Faker-37.3.0, langsmith-0.6.4, asyncio-1.3.0, mock-3.14.1
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

tests/core/llm/test_multi_tier_llm.py::test_query_engine_cot_parsing FAILED [100%]

================================== FAILURES ===================================
________________________ test_query_engine_cot_parsing ________________________

mocker = <pytest_mock.plugin.MockerFixture object at 0x000001F97B66B170>

    @pytest.mark.asyncio
    async def test_query_engine_cot_parsing(mocker):
        """
        Test that QueryEngine correctly parses <thinking> tags and logs them to trace.
        MAXIMUM SCRUTINY: verifies reasoning capture and clean output.
        """
        from src.app.modules.intelligence.query.engine import QueryEngine
        from src.app.modules.intelligence.trace.context import TraceContext
        from langchain_openai import ChatOpenAI
    
        # Mock the LLM response with Chain of Thought
        mock_response = MagicMock()
        mock_response.content = (
            "<thinking>\n"
            "1. Analyzing user profile...\n"
            "2. Checking astrology data...\n"
            "3. Formulating response.\n"
            "</thinking>\n"
            "Here is the final answer."
        )
    
        mocker.patch.object(ChatOpenAI, "ainvoke", new_callable=AsyncMock, return_value=mock_response)
    
        # Mock dependencies
        mock_memory = AsyncMock()
        mock_db = AsyncMock()
        mock_activity_logger = AsyncMock()
    
        # Initialize Engine
        engine = QueryEngine(tier=LLMTier.PREMIUM, memory=mock_memory)
        engine.activity_logger = mock_activity_logger
    
        # CRITICAL: Set attributes accessed by engine
        # engine.llm is already an instance, so we need to mock it properly
        # If LLMConfig.get_llm returns a ChatOpenAI instance, we need to mock its attributes
        engine.llm = AsyncMock()
        engine.llm.model_name = "anthropic/claude-sonnet-4.5"
        engine.llm.ainvoke.return_value = mock_response
    
        # Mock internal methods to isolate _generate_answer logic
        engine._calculate_confidence = MagicMock(return_value=0.95)
    
        # Mock LLMConfig to avoid DB access or NoneType errors
        mocker.patch.object(
            LLMConfig,
            "get_model_info",
            return_value=(
                "anthropic",
                "claude-sonnet-4.5",
                MagicMock(temperature=0.7, cost_per_1k_input=0.0, cost_per_1k_output=0.0),
            ),
        )
        mocker.patch.object(LLMConfig, "estimate_cost", return_value=0.001)
    
        # Create a real TraceContext to verify logging
        trace = TraceContext()
    
        # Create a real TraceContext to verify logging
        trace = TraceContext()
    
        # Call internal generator directly to test parsing logic
        # We mock _build_context_from_data since we are testing _generate_answer logic mostly
        # But since _generate_answer is what contains the parsing logic, we call that.
    
>       answer, confidence = await engine._generate_answer(
        ^^^^^^^^^^^^^^^^^^
            question="Test question", context="Test context", modules=["astrology"], trace_id="test-trace-123", trace=trace
        )
E       TypeError: cannot unpack non-iterable NoneType object

tests\core\llm\test_multi_tier_llm.py:166: TypeError
=========================== short test summary info ===========================
FAILED tests/core/llm/test_multi_tier_llm.py::test_query_engine_cot_parsing
============================== 1 failed in 0.72s ==============================

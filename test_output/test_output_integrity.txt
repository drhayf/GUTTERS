2026-01-24T06:19:31.246484Z [info     ] Registered module: human_design (layer: calculation) [src.app.modules.registry]
2026-01-24T06:19:32.524537Z [info     ] Registered module: numerology (layer: calculation) [src.app.modules.registry]
2026-01-24T06:19:32.582298Z [info     ] Registered module: synthesis (layer: intelligence) [src.app.modules.registry]
2026-01-24T06:19:32.607333Z [info     ] Registered module: query (layer: intelligence) [src.app.modules.registry]
2026-01-24T06:19:33.579244Z [info     ] Registered module: observer (layer: intelligence) [src.app.modules.registry]
============================= test session starts =============================
platform win32 -- Python 3.12.10, pytest-9.0.2, pluggy-1.6.0 -- C:\dev\GUTTERS\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\dev\GUTTERS
configfile: pyproject.toml
plugins: anyio-4.12.1, Faker-37.3.0, langsmith-0.6.4, asyncio-1.3.0, mock-3.14.1
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

tests/integration/test_end_to_end.py::test_complete_user_journey [OK] User created (ID: 56)
[OK] Profile initialized
2026-01-24T06:19:34.998365Z [info     ] Registered module: astrology (layer: calculation) [src.app.modules.registry]
[OK] Calculation modules complete (Astrology, HD, Numerology)
[WARN] Tracking module check skipped/failed (possibly no API key): 'SolarTracker' object has no attribute 'get_current_conditions'
2026-01-24T06:19:38.693569Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
2026-01-24T06:19:44.875191Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
2026-01-24T06:19:51.071824Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
2026-01-24T06:19:55.275489Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
2026-01-24T06:20:04.971217Z [info     ] Stored master synthesis for user 56 with 3 modules [src.app.core.memory.active_memory]
[OK] Master synthesis generated (1791 chars)
[OK] Active Memory operational (Context retrieved)
2026-01-24T06:20:08.805628Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
2026-01-24T06:20:10.017865Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
2026-01-24T06:20:15.669698Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
[OK] Master Chat conversation successful
  Response: # Your Focus for Today

Hello, beautiful Manifestor! 

Today, I invite you to honor **both sides of ...
2026-01-24T06:20:29.169379Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
2026-01-24T06:20:31.387843Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
2026-01-24T06:20:35.689357Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK" [httpx]
[OK] Generative UI component created: multi_slider
[OK] Journal entry created via component
2026-01-24T06:20:52.366149Z [info     ] HTTP Request: POST https://openrouter.ai/api/v1/embeddings "HTTP/1.1 200 OK" [httpx]
FAILED

================================== FAILURES ===================================
_________________________ test_complete_user_journey __________________________

self = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
operation = 'INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id'
parameters = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))

    async def _prepare_and_execute(self, operation, parameters):
        adapt_connection = self._adapt_connection
    
        async with adapt_connection._execute_mutex:
            if not adapt_connection._started:
                await adapt_connection._start_transaction()
    
            if parameters is None:
                parameters = ()
    
            try:
                prepared_stmt, attributes = await adapt_connection._prepare(
                    operation, self._invalidate_schema_cache_asof
                )
    
                if attributes:
                    self.description = [
                        (
                            attr.name,
                            attr.type.oid,
                            None,
                            None,
                            None,
                            None,
                            None,
                        )
                        for attr in attributes
                    ]
                else:
                    self.description = None
    
                if self.server_side:
                    self._cursor = await prepared_stmt.cursor(*parameters)
                    self.rowcount = -1
                else:
>                   self._rows = deque(await prepared_stmt.fetch(*parameters))
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:550: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <asyncpg.prepared_stmt.PreparedStatement object at 0x000002318317D8F0>
timeout = None
args = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))

    @connresource.guarded
    async def fetch(self, *args, timeout=None):
        r"""Execute the statement and return a list of :class:`Record` objects.
    
        :param str query: Query text
        :param args: Query arguments
        :param float timeout: Optional timeout value in seconds.
    
        :return: A list of :class:`Record` instances.
        """
>       data = await self.__bind_execute(args, 0, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\asyncpg\prepared_stmt.py:176: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <asyncpg.prepared_stmt.PreparedStatement object at 0x000002318317D8F0>
args = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))
limit = 0, timeout = None

    async def __bind_execute(self, args, limit, timeout):
>       data, status, _ = await self.__do_execute(
            lambda protocol: protocol.bind_execute(
                self._state, args, '', limit, True, timeout))

.venv\Lib\site-packages\asyncpg\prepared_stmt.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <asyncpg.prepared_stmt.PreparedStatement object at 0x000002318317D8F0>
executor = <function PreparedStatement.__bind_execute.<locals>.<lambda> at 0x0000023183189580>

    async def __do_execute(self, executor):
        protocol = self._connection._protocol
        try:
>           return await executor(protocol)
                   ^^^^^^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\asyncpg\prepared_stmt.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   asyncpg.exceptions.NotNullViolationError: null value in column "content_metadata" of relation "embeddings" violates not-null constraint
E   DETAIL:  Failing row contains (98, 56, Journal Entry (2026-01-24T06:20:51.836019+00:00):
E   Felt anxious d..., [-0.019126607,0.008517893,-0.047176022,0.014768274,0.008930167,0..., null, 2026-01-24 06:20:52.844605+00, 2026-01-24 06:20:52.844605+00).

asyncpg\\protocol\\protocol.pyx:206: NotNullViolationError

The above exception was the direct cause of the following exception:

self = <sqlalchemy.engine.base.Connection object at 0x000002318316DE50>
dialect = <sqlalchemy.dialects.postgresql.asyncpg.PGDialect_asyncpg object at 0x00000231CC971130>
context = <sqlalchemy.dialects.postgresql.asyncpg.PGExecutionContext_asyncpg object at 0x000002318315E960>
statement = <sqlalchemy.dialects.postgresql.asyncpg.PGCompiler_asyncpg object at 0x000002318315E150>
parameters = [(56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\...44605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

.venv\Lib\site-packages\sqlalchemy\engine\base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.PGDialect_asyncpg object at 0x00000231CC971130>
cursor = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
statement = 'INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id'
parameters = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))
context = <sqlalchemy.dialects.postgresql.asyncpg.PGExecutionContext_asyncpg object at 0x000002318315E960>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)

.venv\Lib\site-packages\sqlalchemy\engine\default.py:952: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
operation = 'INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id'
parameters = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))

    def execute(self, operation, parameters=None):
>       self._adapt_connection.await_(
            self._prepare_and_execute(operation, parameters)
        )

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:585: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

awaitable = <coroutine object AsyncAdapt_asyncpg_cursor._prepare_and_execute at 0x000002318306D150>

    def await_only(awaitable: Awaitable[_T]) -> _T:
        """Awaits an async function in a sync method.
    
        The sync method must be inside a :func:`greenlet_spawn` context.
        :func:`await_only` calls cannot be nested.
    
        :param awaitable: The coroutine to call.
    
        """
        # this is called in the context greenlet while running fn
        current = getcurrent()
        if not getattr(current, "__sqlalchemy_greenlet_provider__", False):
            _safe_cancel_awaitable(awaitable)
    
            raise exc.MissingGreenlet(
                "greenlet_spawn has not been called; can't call await_only() "
                "here. Was IO attempted in an unexpected place?"
            )
    
        # returns the control to the driver greenlet passing it
        # a coroutine to run. Once the awaitable is done, the driver greenlet
        # switches back to this greenlet with the result of awaitable that is
        # then returned to the caller (or raised as error)
>       return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\util\_concurrency_py3k.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fn = <bound method Session.commit of <sqlalchemy.orm.session.Session object at 0x00000231829F5BE0>>
_require_await = False, args = (), kwargs = {}
context = <_AsyncIoGreenlet object at 0x0000023183139340 (otid=0x00000231CB7F74E0) dead>
switch_occurred = True
result = <coroutine object AsyncAdapt_asyncpg_connection._rollback_and_discard at 0x00000231830D6260>
value = None

    async def greenlet_spawn(
        fn: Callable[..., _T],
        *args: Any,
        _require_await: bool = False,
        **kwargs: Any,
    ) -> _T:
        """Runs a sync function ``fn`` in a new greenlet.
    
        The sync function can then use :func:`await_only` to wait for async
        functions.
    
        :param fn: The sync callable to call.
        :param \\*args: Positional arguments to pass to the ``fn`` callable.
        :param \\*\\*kwargs: Keyword arguments to pass to the ``fn`` callable.
        """
    
        result: Any
        context = _AsyncIoGreenlet(fn, getcurrent())
        # runs the function synchronously in gl greenlet. If the execution
        # is interrupted by await_only, context is not dead and result is a
        # coroutine to wait. If the context is dead the function has
        # returned, and its result can be returned.
        switch_occurred = False
        result = context.switch(*args, **kwargs)
        while not context.dead:
            switch_occurred = True
            try:
                # wait for a coroutine from await_only and then return its
                # result back to it.
>               value = await result
                        ^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\util\_concurrency_py3k.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
operation = 'INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id'
parameters = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))

    async def _prepare_and_execute(self, operation, parameters):
        adapt_connection = self._adapt_connection
    
        async with adapt_connection._execute_mutex:
            if not adapt_connection._started:
                await adapt_connection._start_transaction()
    
            if parameters is None:
                parameters = ()
    
            try:
                prepared_stmt, attributes = await adapt_connection._prepare(
                    operation, self._invalidate_schema_cache_asof
                )
    
                if attributes:
                    self.description = [
                        (
                            attr.name,
                            attr.type.oid,
                            None,
                            None,
                            None,
                            None,
                            None,
                        )
                        for attr in attributes
                    ]
                else:
                    self.description = None
    
                if self.server_side:
                    self._cursor = await prepared_stmt.cursor(*parameters)
                    self.rowcount = -1
                else:
                    self._rows = deque(await prepared_stmt.fetch(*parameters))
                    status = prepared_stmt.get_statusmsg()
    
                    reg = re.match(
                        r"(?:SELECT|UPDATE|DELETE|INSERT \d+) (\d+)",
                        status or "",
                    )
                    if reg:
                        self.rowcount = int(reg.group(1))
                    else:
                        self.rowcount = -1
    
            except Exception as error:
>               self._handle_exception(error)

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:563: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
error = NotNullViolationError('null value in column "content_metadata" of relation "embeddings" violates not-null constraint')

    def _handle_exception(self, error):
>       self._adapt_connection._handle_exception(error)

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AdaptedConnection <asyncpg.connection.Connection object at 0x0000023183128B90>>
error = NotNullViolationError('null value in column "content_metadata" of relation "embeddings" violates not-null constraint')

    def _handle_exception(self, error):
        if self._connection.is_closed():
            self._transaction = None
            self._started = False
    
        if not isinstance(error, AsyncAdapt_asyncpg_dbapi.Error):
            exception_mapping = self.dbapi._asyncpg_error_translate
    
            for super_ in type(error).__mro__:
                if super_ in exception_mapping:
                    translated_error = exception_mapping[super_](
                        "%s: %s" % (type(error), error)
                    )
                    translated_error.pgcode = translated_error.sqlstate = (
                        getattr(error, "sqlstate", None)
                    )
>                   raise translated_error from error
E                   sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_dbapi.IntegrityError: <class 'asyncpg.exceptions.NotNullViolationError'>: null value in column "content_metadata" of relation "embeddings" violates not-null constraint
E                   DETAIL:  Failing row contains (98, 56, Journal Entry (2026-01-24T06:20:51.836019+00:00):
E                   Felt anxious d..., [-0.019126607,0.008517893,-0.047176022,0.014768274,0.008930167,0..., null, 2026-01-24 06:20:52.844605+00, 2026-01-24 06:20:52.844605+00).

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:797: IntegrityError

The above exception was the direct cause of the following exception:

db = <sqlalchemy.ext.asyncio.session.AsyncSession object at 0x00000231829F5B80>
clean_redis = None

    @pytest.mark.asyncio
    async def test_complete_user_journey(db, clean_redis):
        """
        Complete user journey from registration to AI interaction.
    
        This test verifies:
        1. User creation
        2. Profile initialization
        3. Module calculations
        4. Synthesis generation
        5. Active Memory
        6. Chat conversation
        7. Journal entry
        8. Vector search
        9. Observer detection
        10. Query with full context
        """
    
        # Needs real DB session, not just the fixture, but the fixture 'db' IS a session
        real_db = db
    
        # Initialize Event Bus
        from src.app.core.events.bus import get_event_bus
    
        event_bus = get_event_bus()
        await event_bus.initialize()
    
        # ========================================
        # STEP 1: Create User
        # ========================================
        from src.app.models.user import User
        from src.app.models.user_profile import UserProfile
    
        # Randomize user to avoid DB collisions
        uid = str(uuid.uuid4())[:8]
        user = User(
            name=f"Test Journey {uid}",
            username=f"tj_{uid}",
            email=f"test_journey_{uid}@example.com",
            hashed_password="hashed_secret",
            birth_date=date(1990, 6, 15),
            birth_time=time(14, 30),
            birth_location="New York, NY, USA",
            birth_latitude=40.7128,
            birth_longitude=-74.0060,
            birth_timezone="America/New_York",
        )
    
        real_db.add(user)
        await real_db.commit()
        await real_db.refresh(user)
    
        print(f"[OK] User created (ID: {user.id})")
    
        # Verify profile created
        from sqlalchemy import select
    
        result = await real_db.execute(select(UserProfile).where(UserProfile.user_id == user.id))
        profile = result.scalar_one_or_none()
    
        # If not created by triggers (which might not be present in test DB), create manually
        if not profile:
            profile = UserProfile(user_id=user.id, data={})
            real_db.add(profile)
            await real_db.commit()
            await real_db.refresh(profile)
    
        assert profile is not None
    
        print(f"[OK] Profile initialized")
    
        # ========================================
        # STEP 2: Calculate Profiles (All Modules)
        # ========================================
        from src.app.modules.calculation.astrology.brain.calculator import calculate_natal_chart
        from src.app.modules.calculation.human_design.brain.calculator import HumanDesignCalculator
        from src.app.modules.calculation.numerology.brain.calculator import NumerologyCalculator
    
        # Astrology (Function, Sync)
        # Note: calculate_natal_chart expects specific arguments
        astro_profile = calculate_natal_chart(
            name=user.name,
            birth_date=user.birth_date,
            birth_time=user.birth_time,
            latitude=user.birth_latitude,
            longitude=user.birth_longitude,
            timezone=user.birth_timezone,
        )
    
        profile.data["astrology"] = astro_profile
    
        # Human Design (Class, Sync)
        hd_calc = HumanDesignCalculator()
        hd_profile_obj = hd_calc.calculate_chart(
            name=user.name,
            birth_date=user.birth_date,
            birth_time=user.birth_time,
            latitude=user.birth_latitude,
            longitude=user.birth_longitude,
            timezone=user.birth_timezone,
        )
    
        profile.data["human_design"] = hd_profile_obj.model_dump(mode="json")
    
        # Numerology (Class, Sync)
        num_calc = NumerologyCalculator()
        num_profile_obj = num_calc.calculate_chart(
            name=user.name,  # Full name
            birth_date=user.birth_date,
        )
    
        profile.data["numerology"] = num_profile_obj.model_dump(mode="json")
    
        from sqlalchemy.orm.attributes import flag_modified
    
        flag_modified(profile, "data")
        await real_db.commit()
    
        print(f"[OK] Calculation modules complete (Astrology, HD, Numerology)")
    
        # ========================================
        # STEP 3: Tracking Modules
        # ========================================
        from src.app.modules.tracking.solar.tracker import SolarTracker
    
        # Mocking external calls for tracking to ensure stability if API keys missing
        # BUT the objective says "Tracking Modules Pull Cosmic Data"
        # We will try real first, but fallback or handle errors if no API key
        try:
            solar_tracker = SolarTracker()
            solar_data = await solar_tracker.get_current_conditions()
    
            assert solar_data is not None
            # Keys might vary depending on API response structure
            # assert 'kp_index' in solar_data or 'solar_wind_speed' in solar_data
    
            print(f"[OK] Tracking modules functional (Solar data retrieved)")
        except Exception as e:
            print(f"[WARN] Tracking module check skipped/failed (possibly no API key): {e}")
    
        # ========================================
        # STEP 4: Synthesis Generation
        # ========================================
        from src.app.modules.intelligence.synthesis.synthesizer import ProfileSynthesizer
    
        synth_engine = ProfileSynthesizer()
        # It returns UnifiedProfile object
        synthesis_obj = await synth_engine.synthesize_profile(user.id, real_db)
        synthesis = synthesis_obj.model_dump()
    
        assert synthesis is not None
        assert "synthesis" in synthesis  # The field is named 'synthesis', not 'synthesis_text'
        assert len(synthesis["modules_included"]) >= 3  # Astrology, HD, Numerology
    
        print(f"[OK] Master synthesis generated ({len(synthesis['synthesis'])} chars)")
    
        # ========================================
        # STEP 5: Active Memory
        # ========================================
        from src.app.core.memory import get_active_memory
    
        memory = get_active_memory()
        # Already initialized in clean_redis but good to be safe
        # await memory.initialize()
    
        # Store synthesis - Already done by synthesizer
        # await memory.update_master_synthesis(user.id, synthesis)
    
        # Retrieve context
        context = await memory.get_full_context(user.id)
    
        assert context["synthesis"] is not None
        # Depending on implementation, profiles might be pulled from DB or cache
        # assert context['profiles']['natal_chart'] is not None
    
        print(f"[OK] Active Memory operational (Context retrieved)")
    
        # ========================================
        # STEP 6: Master Chat Conversation
        # ========================================
        from src.app.modules.features.chat.master_chat import MasterChatHandler
        from src.app.modules.intelligence.query.engine import QueryEngine
        from src.app.core.llm.config import get_premium_llm, LLMTier
    
        query_engine = QueryEngine(llm=get_premium_llm(), memory=memory, tier=LLMTier.PREMIUM, enable_generative_ui=True)
    
        handler = MasterChatHandler(query_engine)
    
        # Send message
        response = await handler.send_message(user.id, "What should I focus on today?", real_db)
    
        assert response["message"] is not None
        assert len(response["message"]) > 10  # Reduced length check for robustness
        assert response["session_id"] is not None
    
        print(f"[OK] Master Chat conversation successful")
        print(f"  Response: {response['message'][:100]}...")
    
        # ========================================
        # STEP 7: Journal Entry via Component
        # ========================================
    
        # First, trigger component generation
        response2 = await handler.send_message(
            user.id, "I felt really anxious today during the solar storm", real_db, session_id=response["session_id"]
        )
    
        # Should generate a mood slider component
        if response2.get("component"):
            print(f"[OK] Generative UI component created: {response2['component']['component_type']}")
    
            # Simulate component submission
            from src.app.modules.intelligence.generative_ui.models import ComponentResponse, ComponentType
    
            comp_response = ComponentResponse(
                component_id=response2["component"]["component_id"],
                component_type=ComponentType(response2["component"]["component_type"]),
                slider_values={"mood": 4, "energy": 3, "anxiety": 8},
                submitted_at=datetime.now(dt_timezone.utc),
            )
    
            # Submit via API logic (simulated)
            if "component_responses" not in profile.data:
                profile.data["component_responses"] = []
    
            profile.data["component_responses"].append(comp_response.model_dump(mode="json"))
    
            # Create journal entry
            if "journal_entries" not in profile.data:
                profile.data["journal_entries"] = []
    
            entry = {
                "id": str(uuid.uuid4()),
                "timestamp": datetime.now(dt_timezone.utc).isoformat(),
                "text": "Felt anxious during solar storm",
                "mood_score": 4,
                "energy_score": 3,
                "anxiety_score": 8,
                "source": "multi_slider_component",
            }
    
            profile.data["journal_entries"].append(entry)
            flag_modified(profile, "data")
            await real_db.commit()
    
            print(f"[OK] Journal entry created via component")
        else:
            # Manual journal entry if no component
            if "journal_entries" not in profile.data:
                profile.data["journal_entries"] = []
    
            entry = {
                "id": str(uuid.uuid4()),
                "timestamp": datetime.now(dt_timezone.utc).isoformat(),
                "text": "Felt anxious during solar storm",
                "mood_score": 4,
                "source": "manual",
            }
    
            profile.data["journal_entries"].append(entry)
            flag_modified(profile, "data")
            await real_db.commit()
    
            print(f"[OK] Journal entry created manually")
    
        # ========================================
        # STEP 8: Vector Search
        # ========================================
        from src.app.modules.intelligence.vector.embedding_service import EmbeddingService
        from src.app.modules.intelligence.vector.search_engine import VectorSearchEngine
        from src.app.models.embedding import Embedding
        from src.app.core.config import settings
    
        # Create embedding for journal entry
        embed_service = EmbeddingService(settings.OPENROUTER_API_KEY.get_secret_value())
    
        embedded = await embed_service.embed_journal_entry(entry)
    
        embedding_record = Embedding(
            user_id=user.id,
            content=embedded["content"],
            embedding=embedded["embedding"],
            metadata=embedded["content_metadata"],
        )
    
        real_db.add(embedding_record)
>       await real_db.commit()

tests\integration\test_end_to_end.py:299: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.ext.asyncio.session.AsyncSession object at 0x00000231829F5B80>

    async def commit(self) -> None:
        """Commit the current transaction in progress.
    
        .. seealso::
    
            :meth:`_orm.Session.commit` - main documentation for
            "commit"
        """
>       await greenlet_spawn(self.sync_session.commit)

.venv\Lib\site-packages\sqlalchemy\ext\asyncio\session.py:1000: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fn = <bound method Session.commit of <sqlalchemy.orm.session.Session object at 0x00000231829F5BE0>>
_require_await = False, args = (), kwargs = {}
context = <_AsyncIoGreenlet object at 0x0000023183139340 (otid=0x00000231CB7F74E0) dead>
switch_occurred = True
result = <coroutine object AsyncAdapt_asyncpg_connection._rollback_and_discard at 0x00000231830D6260>
value = None

    async def greenlet_spawn(
        fn: Callable[..., _T],
        *args: Any,
        _require_await: bool = False,
        **kwargs: Any,
    ) -> _T:
        """Runs a sync function ``fn`` in a new greenlet.
    
        The sync function can then use :func:`await_only` to wait for async
        functions.
    
        :param fn: The sync callable to call.
        :param \\*args: Positional arguments to pass to the ``fn`` callable.
        :param \\*\\*kwargs: Keyword arguments to pass to the ``fn`` callable.
        """
    
        result: Any
        context = _AsyncIoGreenlet(fn, getcurrent())
        # runs the function synchronously in gl greenlet. If the execution
        # is interrupted by await_only, context is not dead and result is a
        # coroutine to wait. If the context is dead the function has
        # returned, and its result can be returned.
        switch_occurred = False
        result = context.switch(*args, **kwargs)
        while not context.dead:
            switch_occurred = True
            try:
                # wait for a coroutine from await_only and then return its
                # result back to it.
                value = await result
            except BaseException:
                # this allows an exception to be raised within
                # the moderated greenlet so that it can continue
                # its expected flow.
                result = context.throw(*sys.exc_info())
            else:
>               result = context.switch(value)
                         ^^^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\util\_concurrency_py3k.py:203: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.session.Session object at 0x00000231829F5BE0>

    def commit(self) -> None:
        """Flush pending changes and commit the current transaction.
    
        When the COMMIT operation is complete, all objects are fully
        :term:`expired`, erasing their internal contents, which will be
        automatically re-loaded when the objects are next accessed. In the
        interim, these objects are in an expired state and will not function if
        they are :term:`detached` from the :class:`.Session`. Additionally,
        this re-load operation is not supported when using asyncio-oriented
        APIs. The :paramref:`.Session.expire_on_commit` parameter may be used
        to disable this behavior.
    
        When there is no transaction in place for the :class:`.Session`,
        indicating that no operations were invoked on this :class:`.Session`
        since the previous call to :meth:`.Session.commit`, the method will
        begin and commit an internal-only "logical" transaction, that does not
        normally affect the database unless pending flush changes were
        detected, but will still invoke event handlers and object expiration
        rules.
    
        The outermost database transaction is committed unconditionally,
        automatically releasing any SAVEPOINTs in effect.
    
        .. seealso::
    
            :ref:`session_committing`
    
            :ref:`unitofwork_transaction`
    
            :ref:`asyncio_orm_avoid_lazyloads`
    
        """
        trans = self._transaction
        if trans is None:
            trans = self._autobegin_t()
    
>       trans.commit(_to_root=True)

.venv\Lib\site-packages\sqlalchemy\orm\session.py:2030: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.session.SessionTransaction object at 0x0000023183145010>
_to_root = True

>   ???

<string>:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fn = <function SessionTransaction.commit at 0x00000231CC125940>
self = <sqlalchemy.orm.session.SessionTransaction object at 0x0000023183145010>
arg = (), kw = {'_to_root': True}
current_state = <SessionTransactionState.ACTIVE: 1>
next_state = <_StateChangeStates.ANY: 1>, existing_fn = None
expect_state = <SessionTransactionState.CLOSED: 5>

    @util.decorator
    def _go(fn: _F, self: Any, *arg: Any, **kw: Any) -> Any:
        current_state = self._state
    
        if (
            has_prerequisite_states
            and current_state not in prerequisite_state_collection
        ):
            self._raise_for_prerequisite_state(fn.__name__, current_state)
    
        next_state = self._next_state
        existing_fn = self._current_fn
        expect_state = moves_to if expect_state_change else current_state
    
        if (
            # destination states are restricted
            next_state is not _StateChangeStates.ANY
            # method seeks to change state
            and expect_state_change
            # destination state incorrect
            and next_state is not expect_state
        ):
            if existing_fn and next_state in (
                _StateChangeStates.NO_CHANGE,
                _StateChangeStates.CHANGE_IN_PROGRESS,
            ):
                raise sa_exc.IllegalStateChangeError(
                    f"Method '{fn.__name__}()' can't be called here; "
                    f"method '{existing_fn.__name__}()' is already "
                    f"in progress and this would cause an unexpected "
                    f"state change to {moves_to!r}",
                    code="isce",
                )
            else:
                raise sa_exc.IllegalStateChangeError(
                    f"Can't run operation '{fn.__name__}()' here; "
                    f"will move to state {moves_to!r} where we are "
                    f"expecting {next_state!r}",
                    code="isce",
                )
    
        self._current_fn = fn
        self._next_state = _StateChangeStates.CHANGE_IN_PROGRESS
        try:
>           ret_value = fn(self, *arg, **kw)
                        ^^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\orm\state_changes.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.session.SessionTransaction object at 0x0000023183145010>
_to_root = True

    @_StateChange.declare_states(
        (SessionTransactionState.ACTIVE, SessionTransactionState.PREPARED),
        SessionTransactionState.CLOSED,
    )
    def commit(self, _to_root: bool = False) -> None:
        if self._state is not SessionTransactionState.PREPARED:
            with self._expect_state(SessionTransactionState.PREPARED):
>               self._prepare_impl()

.venv\Lib\site-packages\sqlalchemy\orm\session.py:1311: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.session.SessionTransaction object at 0x0000023183145010>

>   ???

<string>:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fn = <function SessionTransaction._prepare_impl at 0x00000231CC125760>
self = <sqlalchemy.orm.session.SessionTransaction object at 0x0000023183145010>
arg = (), kw = {}, current_state = <SessionTransactionState.ACTIVE: 1>
next_state = <SessionTransactionState.PREPARED: 2>
existing_fn = <function SessionTransaction.commit at 0x00000231CC125940>
expect_state = <SessionTransactionState.PREPARED: 2>

    @util.decorator
    def _go(fn: _F, self: Any, *arg: Any, **kw: Any) -> Any:
        current_state = self._state
    
        if (
            has_prerequisite_states
            and current_state not in prerequisite_state_collection
        ):
            self._raise_for_prerequisite_state(fn.__name__, current_state)
    
        next_state = self._next_state
        existing_fn = self._current_fn
        expect_state = moves_to if expect_state_change else current_state
    
        if (
            # destination states are restricted
            next_state is not _StateChangeStates.ANY
            # method seeks to change state
            and expect_state_change
            # destination state incorrect
            and next_state is not expect_state
        ):
            if existing_fn and next_state in (
                _StateChangeStates.NO_CHANGE,
                _StateChangeStates.CHANGE_IN_PROGRESS,
            ):
                raise sa_exc.IllegalStateChangeError(
                    f"Method '{fn.__name__}()' can't be called here; "
                    f"method '{existing_fn.__name__}()' is already "
                    f"in progress and this would cause an unexpected "
                    f"state change to {moves_to!r}",
                    code="isce",
                )
            else:
                raise sa_exc.IllegalStateChangeError(
                    f"Can't run operation '{fn.__name__}()' here; "
                    f"will move to state {moves_to!r} where we are "
                    f"expecting {next_state!r}",
                    code="isce",
                )
    
        self._current_fn = fn
        self._next_state = _StateChangeStates.CHANGE_IN_PROGRESS
        try:
>           ret_value = fn(self, *arg, **kw)
                        ^^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\orm\state_changes.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.session.SessionTransaction object at 0x0000023183145010>

    @_StateChange.declare_states(
        (SessionTransactionState.ACTIVE,), SessionTransactionState.PREPARED
    )
    def _prepare_impl(self) -> None:
        if self._parent is None or self.nested:
            self.session.dispatch.before_commit(self.session)
    
        stx = self.session._transaction
        assert stx is not None
        if stx is not self:
            for subtransaction in stx._iterate_self_and_parents(upto=self):
                subtransaction.commit()
    
        if not self.session._flushing:
            for _flush_guard in range(100):
                if self.session._is_clean():
                    break
>               self.session.flush()

.venv\Lib\site-packages\sqlalchemy\orm\session.py:1286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.session.Session object at 0x00000231829F5BE0>
objects = None

    def flush(self, objects: Optional[Sequence[Any]] = None) -> None:
        """Flush all the object changes to the database.
    
        Writes out all pending object creations, deletions and modifications
        to the database as INSERTs, DELETEs, UPDATEs, etc.  Operations are
        automatically ordered by the Session's unit of work dependency
        solver.
    
        Database operations will be issued in the current transactional
        context and do not affect the state of the transaction, unless an
        error occurs, in which case the entire transaction is rolled back.
        You may flush() as often as you like within a transaction to move
        changes from Python to the database's transaction buffer.
    
        :param objects: Optional; restricts the flush operation to operate
          only on elements that are in the given collection.
    
          This feature is for an extremely narrow set of use cases where
          particular objects may need to be operated upon before the
          full flush() occurs.  It is not intended for general use.
    
        """
    
        if self._flushing:
            raise sa_exc.InvalidRequestError("Session is already flushing")
    
        if self._is_clean():
            return
        try:
            self._flushing = True
>           self._flush(objects)

.venv\Lib\site-packages\sqlalchemy\orm\session.py:4331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.session.Session object at 0x00000231829F5BE0>
objects = None

    def _flush(self, objects: Optional[Sequence[object]] = None) -> None:
        dirty = self._dirty_states
        if not dirty and not self._deleted and not self._new:
            self.identity_map._modified.clear()
            return
    
        flush_context = UOWTransaction(self)
    
        if self.dispatch.before_flush:
            self.dispatch.before_flush(self, flush_context, objects)
            # re-establish "dirty states" in case the listeners
            # added
            dirty = self._dirty_states
    
        deleted = set(self._deleted)
        new = set(self._new)
    
        dirty = set(dirty).difference(deleted)
    
        # create the set of all objects we want to operate upon
        if objects:
            # specific list passed in
            objset = set()
            for o in objects:
                try:
                    state = attributes.instance_state(o)
    
                except exc.NO_STATE as err:
                    raise exc.UnmappedInstanceError(o) from err
                objset.add(state)
        else:
            objset = None
    
        # store objects whose fate has been decided
        processed = set()
    
        # put all saves/updates into the flush context.  detect top-level
        # orphans and throw them into deleted.
        if objset:
            proc = new.union(dirty).intersection(objset).difference(deleted)
        else:
            proc = new.union(dirty).difference(deleted)
    
        for state in proc:
            is_orphan = _state_mapper(state)._is_orphan(state)
    
            is_persistent_orphan = is_orphan and state.has_identity
    
            if (
                is_orphan
                and not is_persistent_orphan
                and state._orphaned_outside_of_session
            ):
                self._expunge_states([state])
            else:
                _reg = flush_context.register_object(
                    state, isdelete=is_persistent_orphan
                )
                assert _reg, "Failed to add object to the flush context!"
                processed.add(state)
    
        # put all remaining deletes into the flush context.
        if objset:
            proc = deleted.intersection(objset).difference(processed)
        else:
            proc = deleted.difference(processed)
        for state in proc:
            _reg = flush_context.register_object(state, isdelete=True)
            assert _reg, "Failed to add object to the flush context!"
    
        if not flush_context.has_work:
            return
    
        flush_context.transaction = transaction = self._autobegin_t()._begin()
        try:
            self._warn_on_events = True
            try:
                flush_context.execute()
            finally:
                self._warn_on_events = False
    
            self.dispatch.after_flush(self, flush_context)
    
            flush_context.finalize_flush_changes()
    
            if not objects and self.identity_map._modified:
                len_ = len(self.identity_map._modified)
    
                statelib.InstanceState._commit_all_states(
                    [
                        (state, state.dict)
                        for state in self.identity_map._modified
                    ],
                    instance_dict=self.identity_map,
                )
                util.warn(
                    "Attribute history events accumulated on %d "
                    "previously clean instances "
                    "within inner-flush event handlers have been "
                    "reset, and will not result in database updates. "
                    "Consider using set_committed_value() within "
                    "inner-flush event handlers to avoid this warning." % len_
                )
    
            # useful assertions:
            # if not objects:
            #    assert not self.identity_map._modified
            # else:
            #    assert self.identity_map._modified == \
            #            self.identity_map._modified.difference(objects)
    
            self.dispatch.after_flush_postexec(self, flush_context)
    
            transaction.commit()
    
        except:
>           with util.safe_reraise():
                 ^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\orm\session.py:4466: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.util.langhelpers.safe_reraise object at 0x000002318316D090>
type_ = None, value = None, traceback = None

    def __exit__(
        self,
        type_: Optional[Type[BaseException]],
        value: Optional[BaseException],
        traceback: Optional[types.TracebackType],
    ) -> NoReturn:
        assert self._exc_info is not None
        # see #2703 for notes
        if type_ is None:
            exc_type, exc_value, exc_tb = self._exc_info
            assert exc_value is not None
            self._exc_info = None  # remove potential circular references
>           raise exc_value.with_traceback(exc_tb)

.venv\Lib\site-packages\sqlalchemy\util\langhelpers.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.session.Session object at 0x00000231829F5BE0>
objects = None

    def _flush(self, objects: Optional[Sequence[object]] = None) -> None:
        dirty = self._dirty_states
        if not dirty and not self._deleted and not self._new:
            self.identity_map._modified.clear()
            return
    
        flush_context = UOWTransaction(self)
    
        if self.dispatch.before_flush:
            self.dispatch.before_flush(self, flush_context, objects)
            # re-establish "dirty states" in case the listeners
            # added
            dirty = self._dirty_states
    
        deleted = set(self._deleted)
        new = set(self._new)
    
        dirty = set(dirty).difference(deleted)
    
        # create the set of all objects we want to operate upon
        if objects:
            # specific list passed in
            objset = set()
            for o in objects:
                try:
                    state = attributes.instance_state(o)
    
                except exc.NO_STATE as err:
                    raise exc.UnmappedInstanceError(o) from err
                objset.add(state)
        else:
            objset = None
    
        # store objects whose fate has been decided
        processed = set()
    
        # put all saves/updates into the flush context.  detect top-level
        # orphans and throw them into deleted.
        if objset:
            proc = new.union(dirty).intersection(objset).difference(deleted)
        else:
            proc = new.union(dirty).difference(deleted)
    
        for state in proc:
            is_orphan = _state_mapper(state)._is_orphan(state)
    
            is_persistent_orphan = is_orphan and state.has_identity
    
            if (
                is_orphan
                and not is_persistent_orphan
                and state._orphaned_outside_of_session
            ):
                self._expunge_states([state])
            else:
                _reg = flush_context.register_object(
                    state, isdelete=is_persistent_orphan
                )
                assert _reg, "Failed to add object to the flush context!"
                processed.add(state)
    
        # put all remaining deletes into the flush context.
        if objset:
            proc = deleted.intersection(objset).difference(processed)
        else:
            proc = deleted.difference(processed)
        for state in proc:
            _reg = flush_context.register_object(state, isdelete=True)
            assert _reg, "Failed to add object to the flush context!"
    
        if not flush_context.has_work:
            return
    
        flush_context.transaction = transaction = self._autobegin_t()._begin()
        try:
            self._warn_on_events = True
            try:
>               flush_context.execute()

.venv\Lib\site-packages\sqlalchemy\orm\session.py:4427: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.orm.unitofwork.UOWTransaction object at 0x000002318316D460>

    def execute(self) -> None:
        postsort_actions = self._generate_actions()
    
        postsort_actions = sorted(
            postsort_actions,
            key=lambda item: item.sort_key,
        )
        # sort = topological.sort(self.dependencies, postsort_actions)
        # print "--------------"
        # print "\ndependencies:", self.dependencies
        # print "\ncycles:", self.cycles
        # print "\nsort:", list(sort)
        # print "\nCOUNT OF POSTSORT ACTIONS", len(postsort_actions)
    
        # execute
        if self.cycles:
            for subset in topological.sort_as_subsets(
                self.dependencies, postsort_actions
            ):
                set_ = set(subset)
                while set_:
                    n = set_.pop()
                    n.execute_aggregate(self, set_)
        else:
            for rec in topological.sort(self.dependencies, postsort_actions):
>               rec.execute(self)

.venv\Lib\site-packages\sqlalchemy\orm\unitofwork.py:466: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = SaveUpdateAll(Mapper[Embedding(embeddings)])
uow = <sqlalchemy.orm.unitofwork.UOWTransaction object at 0x000002318316D460>

    @util.preload_module("sqlalchemy.orm.persistence")
    def execute(self, uow):
>       util.preloaded.orm_persistence.save_obj(
            self.mapper,
            uow.states_for_mapper_hierarchy(self.mapper, False, False),
            uow,
        )

.venv\Lib\site-packages\sqlalchemy\orm\unitofwork.py:642: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

base_mapper = <Mapper at 0x231e8bca120; Embedding>
states = <generator object UOWTransaction.states_for_mapper_hierarchy at 0x00000231831287C0>
uowtransaction = <sqlalchemy.orm.unitofwork.UOWTransaction object at 0x000002318316D460>
single = False

    def save_obj(base_mapper, states, uowtransaction, single=False):
        """Issue ``INSERT`` and/or ``UPDATE`` statements for a list
        of objects.
    
        This is called within the context of a UOWTransaction during a
        flush operation, given a list of states to be flushed.  The
        base mapper in an inheritance hierarchy handles the inserts/
        updates for all descendant mappers.
    
        """
    
        # if batch=false, call _save_obj separately for each object
        if not single and not base_mapper.batch:
            for state in _sort_states(base_mapper, states):
                save_obj(base_mapper, [state], uowtransaction, single=True)
            return
    
        states_to_update = []
        states_to_insert = []
    
        for (
            state,
            dict_,
            mapper,
            connection,
            has_identity,
            row_switch,
            update_version_id,
        ) in _organize_states_for_save(base_mapper, states, uowtransaction):
            if has_identity or row_switch:
                states_to_update.append(
                    (state, dict_, mapper, connection, update_version_id)
                )
            else:
                states_to_insert.append((state, dict_, mapper, connection))
    
        for table, mapper in base_mapper._sorted_tables.items():
            if table not in mapper._pks_by_table:
                continue
            insert = _collect_insert_commands(table, states_to_insert)
    
            update = _collect_update_commands(
                uowtransaction, table, states_to_update
            )
    
            _emit_update_statements(
                base_mapper,
                uowtransaction,
                mapper,
                table,
                update,
            )
    
>           _emit_insert_statements(
                base_mapper,
                uowtransaction,
                mapper,
                table,
                insert,
            )

.venv\Lib\site-packages\sqlalchemy\orm\persistence.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

base_mapper = <Mapper at 0x231e8bca120; Embedding>
uowtransaction = <sqlalchemy.orm.unitofwork.UOWTransaction object at 0x000002318316D460>
mapper = <Mapper at 0x231e8bca120; Embedding>
table = Table('embeddings', MetaData(), Column('id', Integer(), table=<embeddings>, primary_key=True, nullable=False), Column(...0x00000231E8C71940>), default=CallableColumnDefault(<function Embedding.<lambda> at 0x00000231E8C718A0>)), schema=None)
insert = <generator object _collect_insert_commands at 0x0000023183053A10>

    def _emit_insert_statements(
        base_mapper,
        uowtransaction,
        mapper,
        table,
        insert,
        *,
        bookkeeping=True,
        use_orm_insert_stmt=None,
        execution_options=None,
    ):
        """Emit INSERT statements corresponding to value lists collected
        by _collect_insert_commands()."""
    
        if use_orm_insert_stmt is not None:
            cached_stmt = use_orm_insert_stmt
            exec_opt = util.EMPTY_DICT
    
            # if a user query with RETURNING was passed, we definitely need
            # to use RETURNING.
            returning_is_required_anyway = bool(use_orm_insert_stmt._returning)
            deterministic_results_reqd = (
                returning_is_required_anyway
                and use_orm_insert_stmt._sort_by_parameter_order
            ) or bookkeeping
        else:
            returning_is_required_anyway = False
            deterministic_results_reqd = bookkeeping
            cached_stmt = base_mapper._memo(("insert", table), table.insert)
            exec_opt = {"compiled_cache": base_mapper._compiled_cache}
    
        if execution_options:
            execution_options = util.EMPTY_DICT.merge_with(
                exec_opt, execution_options
            )
        else:
            execution_options = exec_opt
    
        return_result = None
    
        for (
            (connection, _, hasvalue, has_all_pks, has_all_defaults),
            records,
        ) in groupby(
            insert,
            lambda rec: (
                rec[4],  # connection
                set(rec[2]),  # parameter keys
                bool(rec[5]),  # whether we have "value" parameters
                rec[6],
                rec[7],
            ),
        ):
            statement = cached_stmt
    
            if use_orm_insert_stmt is not None:
                statement = statement._annotate(
                    {
                        "_emit_insert_table": table,
                        "_emit_insert_mapper": mapper,
                    }
                )
    
            if (
                (
                    not bookkeeping
                    or (
                        has_all_defaults
                        or not base_mapper._prefer_eager_defaults(
                            connection.dialect, table
                        )
                        or not table.implicit_returning
                        or not connection.dialect.insert_returning
                    )
                )
                and not returning_is_required_anyway
                and has_all_pks
                and not hasvalue
            ):
                # the "we don't need newly generated values back" section.
                # here we have all the PKs, all the defaults or we don't want
                # to fetch them, or the dialect doesn't support RETURNING at all
                # so we have to post-fetch / use lastrowid anyway.
                records = list(records)
                multiparams = [rec[2] for rec in records]
    
                result = connection.execute(
                    statement, multiparams, execution_options=execution_options
                )
                if bookkeeping:
                    for (
                        (
                            state,
                            state_dict,
                            params,
                            mapper_rec,
                            conn,
                            value_params,
                            has_all_pks,
                            has_all_defaults,
                        ),
                        last_inserted_params,
                    ) in zip(records, result.context.compiled_parameters):
                        if state:
                            _postfetch(
                                mapper_rec,
                                uowtransaction,
                                table,
                                state,
                                state_dict,
                                result,
                                last_inserted_params,
                                value_params,
                                False,
                                (
                                    result.returned_defaults
                                    if not result.context.executemany
                                    else None
                                ),
                            )
                        else:
                            _postfetch_bulk_save(mapper_rec, state_dict, table)
    
            else:
                # here, we need defaults and/or pk values back or we otherwise
                # know that we are using RETURNING in any case
    
                records = list(records)
    
                if returning_is_required_anyway or (
                    table.implicit_returning and not hasvalue and len(records) > 1
                ):
                    if (
                        deterministic_results_reqd
                        and connection.dialect.insert_executemany_returning_sort_by_parameter_order  # noqa: E501
                    ) or (
                        not deterministic_results_reqd
                        and connection.dialect.insert_executemany_returning
                    ):
                        do_executemany = True
                    elif returning_is_required_anyway:
                        if deterministic_results_reqd:
                            dt = " with RETURNING and sort by parameter order"
                        else:
                            dt = " with RETURNING"
                        raise sa_exc.InvalidRequestError(
                            f"Can't use explicit RETURNING for bulk INSERT "
                            f"operation with "
                            f"{connection.dialect.dialect_description} backend; "
                            f"executemany{dt} is not enabled for this dialect."
                        )
                    else:
                        do_executemany = False
                else:
                    do_executemany = False
    
                if use_orm_insert_stmt is None:
                    if (
                        not has_all_defaults
                        and base_mapper._prefer_eager_defaults(
                            connection.dialect, table
                        )
                    ):
                        statement = statement.return_defaults(
                            *mapper._server_default_cols[table],
                            sort_by_parameter_order=bookkeeping,
                        )
    
                if mapper.version_id_col is not None:
                    statement = statement.return_defaults(
                        mapper.version_id_col,
                        sort_by_parameter_order=bookkeeping,
                    )
                elif do_executemany:
                    statement = statement.return_defaults(
                        *table.primary_key, sort_by_parameter_order=bookkeeping
                    )
    
                if do_executemany:
                    multiparams = [rec[2] for rec in records]
    
                    result = connection.execute(
                        statement, multiparams, execution_options=execution_options
                    )
    
                    if use_orm_insert_stmt is not None:
                        if return_result is None:
                            return_result = result
                        else:
                            return_result = return_result.splice_vertically(result)
    
                    if bookkeeping:
                        for (
                            (
                                state,
                                state_dict,
                                params,
                                mapper_rec,
                                conn,
                                value_params,
                                has_all_pks,
                                has_all_defaults,
                            ),
                            last_inserted_params,
                            inserted_primary_key,
                            returned_defaults,
                        ) in zip_longest(
                            records,
                            result.context.compiled_parameters,
                            result.inserted_primary_key_rows,
                            result.returned_defaults_rows or (),
                        ):
                            if inserted_primary_key is None:
                                # this is a real problem and means that we didn't
                                # get back as many PK rows.  we can't continue
                                # since this indicates PK rows were missing, which
                                # means we likely mis-populated records starting
                                # at that point with incorrectly matched PK
                                # values.
                                raise orm_exc.FlushError(
                                    "Multi-row INSERT statement for %s did not "
                                    "produce "
                                    "the correct number of INSERTed rows for "
                                    "RETURNING.  Ensure there are no triggers or "
                                    "special driver issues preventing INSERT from "
                                    "functioning properly." % mapper_rec
                                )
    
                            for pk, col in zip(
                                inserted_primary_key,
                                mapper._pks_by_table[table],
                            ):
                                prop = mapper_rec._columntoproperty[col]
                                if state_dict.get(prop.key) is None:
                                    state_dict[prop.key] = pk
    
                            if state:
                                _postfetch(
                                    mapper_rec,
                                    uowtransaction,
                                    table,
                                    state,
                                    state_dict,
                                    result,
                                    last_inserted_params,
                                    value_params,
                                    False,
                                    returned_defaults,
                                )
                            else:
                                _postfetch_bulk_save(mapper_rec, state_dict, table)
                else:
                    assert not returning_is_required_anyway
    
                    for (
                        state,
                        state_dict,
                        params,
                        mapper_rec,
                        connection,
                        value_params,
                        has_all_pks,
                        has_all_defaults,
                    ) in records:
                        if value_params:
                            result = connection.execute(
                                statement.values(value_params),
                                params,
                                execution_options=execution_options,
                            )
                        else:
>                           result = connection.execute(
                                statement,
                                params,
                                execution_options=execution_options,
                            )

.venv\Lib\site-packages\sqlalchemy\orm\persistence.py:1233: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.engine.base.Connection object at 0x000002318316DE50>
statement = <sqlalchemy.sql.dml.Insert object at 0x000002318316EB10>
parameters = {'content': 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy:...03123951, -0.047176022082567215, 0.014768273569643497, 0.008930167183279991, 0.027990516275167465, ...], 'user_id': 56}

    def execute(
        self,
        statement: Executable,
        parameters: Optional[_CoreAnyExecuteParams] = None,
        *,
        execution_options: Optional[CoreExecuteOptionsParameter] = None,
    ) -> CursorResult[Any]:
        r"""Executes a SQL statement construct and returns a
        :class:`_engine.CursorResult`.
    
        :param statement: The statement to be executed.  This is always
         an object that is in both the :class:`_expression.ClauseElement` and
         :class:`_expression.Executable` hierarchies, including:
    
         * :class:`_expression.Select`
         * :class:`_expression.Insert`, :class:`_expression.Update`,
           :class:`_expression.Delete`
         * :class:`_expression.TextClause` and
           :class:`_expression.TextualSelect`
         * :class:`_schema.DDL` and objects which inherit from
           :class:`_schema.ExecutableDDLElement`
    
        :param parameters: parameters which will be bound into the statement.
         This may be either a dictionary of parameter names to values,
         or a mutable sequence (e.g. a list) of dictionaries.  When a
         list of dictionaries is passed, the underlying statement execution
         will make use of the DBAPI ``cursor.executemany()`` method.
         When a single dictionary is passed, the DBAPI ``cursor.execute()``
         method will be used.
    
        :param execution_options: optional dictionary of execution options,
         which will be associated with the statement execution.  This
         dictionary can provide a subset of the options that are accepted
         by :meth:`_engine.Connection.execution_options`.
    
        :return: a :class:`_engine.Result` object.
    
        """
        distilled_parameters = _distill_params_20(parameters)
        try:
            meth = statement._execute_on_connection
        except AttributeError as err:
            raise exc.ObjectNotExecutableError(statement) from err
        else:
>           return meth(
                self,
                distilled_parameters,
                execution_options or NO_OPTIONS,
            )

.venv\Lib\site-packages\sqlalchemy\engine\base.py:1419: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.sql.dml.Insert object at 0x000002318316EB10>
connection = <sqlalchemy.engine.base.Connection object at 0x000002318316DE50>
distilled_params = [{'content': 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy...3123951, -0.047176022082567215, 0.014768273569643497, 0.008930167183279991, 0.027990516275167465, ...], 'user_id': 56}]
execution_options = {'compiled_cache': <sqlalchemy.util._collections.LRUCache object at 0x000002318317D580>}

    def _execute_on_connection(
        self,
        connection: Connection,
        distilled_params: _CoreMultiExecuteParams,
        execution_options: CoreExecuteOptionsParameter,
    ) -> Result[Any]:
        if self.supports_execution:
            if TYPE_CHECKING:
                assert isinstance(self, Executable)
>           return connection._execute_clauseelement(
                self, distilled_params, execution_options
            )

.venv\Lib\site-packages\sqlalchemy\sql\elements.py:527: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.engine.base.Connection object at 0x000002318316DE50>
elem = <sqlalchemy.sql.dml.Insert object at 0x000002318316EB10>
distilled_parameters = [{'content': 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy...3123951, -0.047176022082567215, 0.014768273569643497, 0.008930167183279991, 0.027990516275167465, ...], 'user_id': 56}]
execution_options = immutabledict({'compiled_cache': <sqlalchemy.util._collections.LRUCache object at 0x000002318317D580>})

    def _execute_clauseelement(
        self,
        elem: Executable,
        distilled_parameters: _CoreMultiExecuteParams,
        execution_options: CoreExecuteOptionsParameter,
    ) -> CursorResult[Any]:
        """Execute a sql.ClauseElement object."""
    
        execution_options = elem._execution_options.merge_with(
            self._execution_options, execution_options
        )
    
        has_events = self._has_events or self.engine._has_events
        if has_events:
            (
                elem,
                distilled_parameters,
                event_multiparams,
                event_params,
            ) = self._invoke_before_exec_event(
                elem, distilled_parameters, execution_options
            )
    
        if distilled_parameters:
            # ensure we don't retain a link to the view object for keys()
            # which links to the values, which we don't want to cache
            keys = sorted(distilled_parameters[0])
            for_executemany = len(distilled_parameters) > 1
        else:
            keys = []
            for_executemany = False
    
        dialect = self.dialect
    
        schema_translate_map = execution_options.get(
            "schema_translate_map", None
        )
    
        compiled_cache: Optional[CompiledCacheType] = execution_options.get(
            "compiled_cache", self.engine._compiled_cache
        )
    
        compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(
            dialect=dialect,
            compiled_cache=compiled_cache,
            column_keys=keys,
            for_executemany=for_executemany,
            schema_translate_map=schema_translate_map,
            linting=self.dialect.compiler_linting | compiler.WARN_LINTING,
        )
>       ret = self._execute_context(
            dialect,
            dialect.execution_ctx_cls._init_compiled,
            compiled_sql,
            distilled_parameters,
            execution_options,
            compiled_sql,
            distilled_parameters,
            elem,
            extracted_params,
            cache_hit=cache_hit,
        )

.venv\Lib\site-packages\sqlalchemy\engine\base.py:1641: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.engine.base.Connection object at 0x000002318316DE50>
dialect = <sqlalchemy.dialects.postgresql.asyncpg.PGDialect_asyncpg object at 0x00000231CC971130>
constructor = <bound method DefaultExecutionContext._init_compiled of <class 'sqlalchemy.dialects.postgresql.asyncpg.PGExecutionContext_asyncpg'>>
statement = <sqlalchemy.dialects.postgresql.asyncpg.PGCompiler_asyncpg object at 0x000002318315E150>
parameters = [{'content': 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy...3123951, -0.047176022082567215, 0.014768273569643497, 0.008930167183279991, 0.027990516275167465, ...], 'user_id': 56}]
execution_options = immutabledict({'compiled_cache': <sqlalchemy.util._collections.LRUCache object at 0x000002318317D580>})
args = (<sqlalchemy.dialects.postgresql.asyncpg.PGCompiler_asyncpg object at 0x000002318315E150>, [{'content': 'Journal Entry...67183279991, 0.027990516275167465, ...], 'user_id': 56}], <sqlalchemy.sql.dml.Insert object at 0x000002318316EB10>, [])
kw = {'cache_hit': <CacheStats.CACHE_MISS: 1>}, yp = None
conn = <sqlalchemy.pool.base._ConnectionFairy object at 0x00000231830DF470>
context = <sqlalchemy.dialects.postgresql.asyncpg.PGExecutionContext_asyncpg object at 0x000002318315E960>

    def _execute_context(
        self,
        dialect: Dialect,
        constructor: Callable[..., ExecutionContext],
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
        execution_options: _ExecuteOptions,
        *args: Any,
        **kw: Any,
    ) -> CursorResult[Any]:
        """Create an :class:`.ExecutionContext` and execute, returning
        a :class:`_engine.CursorResult`."""
    
        if execution_options:
            yp = execution_options.get("yield_per", None)
            if yp:
                execution_options = execution_options.union(
                    {"stream_results": True, "max_row_buffer": yp}
                )
        try:
            conn = self._dbapi_connection
            if conn is None:
                conn = self._revalidate_connection()
    
            context = constructor(
                dialect, self, conn, execution_options, *args, **kw
            )
        except (exc.PendingRollbackError, exc.ResourceClosedError):
            raise
        except BaseException as e:
            self._handle_dbapi_exception(
                e, str(statement), parameters, None, None
            )
    
        if (
            self._transaction
            and not self._transaction.is_active
            or (
                self._nested_transaction
                and not self._nested_transaction.is_active
            )
        ):
            self._invalid_transaction()
    
        elif self._trans_context_manager:
            TransactionalContext._trans_ctx_check(self)
    
        if self._transaction is None:
            self._autobegin()
    
        context.pre_exec()
    
        if context.execute_style is ExecuteStyle.INSERTMANYVALUES:
            return self._exec_insertmany_context(dialect, context)
        else:
>           return self._exec_single_context(
                dialect, context, statement, parameters
            )

.venv\Lib\site-packages\sqlalchemy\engine\base.py:1846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.engine.base.Connection object at 0x000002318316DE50>
dialect = <sqlalchemy.dialects.postgresql.asyncpg.PGDialect_asyncpg object at 0x00000231CC971130>
context = <sqlalchemy.dialects.postgresql.asyncpg.PGExecutionContext_asyncpg object at 0x000002318315E960>
statement = <sqlalchemy.dialects.postgresql.asyncpg.PGCompiler_asyncpg object at 0x000002318315E150>
parameters = [(56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\...44605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )
    
            if self._has_events or self.engine._has_events:
                self.dispatch.after_cursor_execute(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
            context.post_exec()
    
            result = context._setup_result_proxy()
    
        except BaseException as e:
>           self._handle_dbapi_exception(
                e, str_statement, effective_parameters, cursor, context
            )

.venv\Lib\site-packages\sqlalchemy\engine\base.py:1986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.engine.base.Connection object at 0x000002318316DE50>
e = IntegrityError('<class \'asyncpg.exceptions.NotNullViolationError\'>: null value in column "content_metadata" of relat...17893,-0.047176022,0.014768274,0.008930167,0..., null, 2026-01-24 06:20:52.844605+00, 2026-01-24 06:20:52.844605+00).')
statement = 'INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id'
parameters = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))
cursor = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
context = <sqlalchemy.dialects.postgresql.asyncpg.PGExecutionContext_asyncpg object at 0x000002318315E960>
is_sub_exec = False

    def _handle_dbapi_exception(
        self,
        e: BaseException,
        statement: Optional[str],
        parameters: Optional[_AnyExecuteParams],
        cursor: Optional[DBAPICursor],
        context: Optional[ExecutionContext],
        is_sub_exec: bool = False,
    ) -> NoReturn:
        exc_info = sys.exc_info()
    
        is_exit_exception = util.is_exit_exception(e)
    
        if not self._is_disconnect:
            self._is_disconnect = (
                isinstance(e, self.dialect.loaded_dbapi.Error)
                and not self.closed
                and self.dialect.is_disconnect(
                    e,
                    self._dbapi_connection if not self.invalidated else None,
                    cursor,
                )
            ) or (is_exit_exception and not self.closed)
    
        invalidate_pool_on_disconnect = not is_exit_exception
    
        ismulti: bool = (
            not is_sub_exec and context.executemany
            if context is not None
            else False
        )
        if self._reentrant_error:
            raise exc.DBAPIError.instance(
                statement,
                parameters,
                e,
                self.dialect.loaded_dbapi.Error,
                hide_parameters=self.engine.hide_parameters,
                dialect=self.dialect,
                ismulti=ismulti,
            ).with_traceback(exc_info[2]) from e
        self._reentrant_error = True
        try:
            # non-DBAPI error - if we already got a context,
            # or there's no string statement, don't wrap it
            should_wrap = isinstance(e, self.dialect.loaded_dbapi.Error) or (
                statement is not None
                and context is None
                and not is_exit_exception
            )
    
            if should_wrap:
                sqlalchemy_exception = exc.DBAPIError.instance(
                    statement,
                    parameters,
                    cast(Exception, e),
                    self.dialect.loaded_dbapi.Error,
                    hide_parameters=self.engine.hide_parameters,
                    connection_invalidated=self._is_disconnect,
                    dialect=self.dialect,
                    ismulti=ismulti,
                )
            else:
                sqlalchemy_exception = None
    
            newraise = None
    
            if (self.dialect._has_events) and not self._execution_options.get(
                "skip_user_error_events", False
            ):
                ctx = ExceptionContextImpl(
                    e,
                    sqlalchemy_exception,
                    self.engine,
                    self.dialect,
                    self,
                    cursor,
                    statement,
                    parameters,
                    context,
                    self._is_disconnect,
                    invalidate_pool_on_disconnect,
                    False,
                )
    
                for fn in self.dialect.dispatch.handle_error:
                    try:
                        # handler returns an exception;
                        # call next handler in a chain
                        per_fn = fn(ctx)
                        if per_fn is not None:
                            ctx.chained_exception = newraise = per_fn
                    except Exception as _raised:
                        # handler raises an exception - stop processing
                        newraise = _raised
                        break
    
                if self._is_disconnect != ctx.is_disconnect:
                    self._is_disconnect = ctx.is_disconnect
                    if sqlalchemy_exception:
                        sqlalchemy_exception.connection_invalidated = (
                            ctx.is_disconnect
                        )
    
                # set up potentially user-defined value for
                # invalidate pool.
                invalidate_pool_on_disconnect = (
                    ctx.invalidate_pool_on_disconnect
                )
    
            if should_wrap and context:
                context.handle_dbapi_exception(e)
    
            if not self._is_disconnect:
                if cursor:
                    self._safe_close_cursor(cursor)
                # "autorollback" was mostly relevant in 1.x series.
                # It's very unlikely to reach here, as the connection
                # does autobegin so when we are here, we are usually
                # in an explicit / semi-explicit transaction.
                # however we have a test which manufactures this
                # scenario in any case using an event handler.
                # test/engine/test_execute.py-> test_actual_autorollback
                if not self.in_transaction():
                    self._rollback_impl()
    
            if newraise:
                raise newraise.with_traceback(exc_info[2]) from e
            elif should_wrap:
                assert sqlalchemy_exception is not None
>               raise sqlalchemy_exception.with_traceback(exc_info[2]) from e

.venv\Lib\site-packages\sqlalchemy\engine\base.py:2363: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.engine.base.Connection object at 0x000002318316DE50>
dialect = <sqlalchemy.dialects.postgresql.asyncpg.PGDialect_asyncpg object at 0x00000231CC971130>
context = <sqlalchemy.dialects.postgresql.asyncpg.PGExecutionContext_asyncpg object at 0x000002318315E960>
statement = <sqlalchemy.dialects.postgresql.asyncpg.PGCompiler_asyncpg object at 0x000002318315E150>
parameters = [(56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\...44605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

.venv\Lib\site-packages\sqlalchemy\engine\base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.PGDialect_asyncpg object at 0x00000231CC971130>
cursor = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
statement = 'INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id'
parameters = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))
context = <sqlalchemy.dialects.postgresql.asyncpg.PGExecutionContext_asyncpg object at 0x000002318315E960>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)

.venv\Lib\site-packages\sqlalchemy\engine\default.py:952: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
operation = 'INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id'
parameters = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))

    def execute(self, operation, parameters=None):
>       self._adapt_connection.await_(
            self._prepare_and_execute(operation, parameters)
        )

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:585: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

awaitable = <coroutine object AsyncAdapt_asyncpg_cursor._prepare_and_execute at 0x000002318306D150>

    def await_only(awaitable: Awaitable[_T]) -> _T:
        """Awaits an async function in a sync method.
    
        The sync method must be inside a :func:`greenlet_spawn` context.
        :func:`await_only` calls cannot be nested.
    
        :param awaitable: The coroutine to call.
    
        """
        # this is called in the context greenlet while running fn
        current = getcurrent()
        if not getattr(current, "__sqlalchemy_greenlet_provider__", False):
            _safe_cancel_awaitable(awaitable)
    
            raise exc.MissingGreenlet(
                "greenlet_spawn has not been called; can't call await_only() "
                "here. Was IO attempted in an unexpected place?"
            )
    
        # returns the control to the driver greenlet passing it
        # a coroutine to run. Once the awaitable is done, the driver greenlet
        # switches back to this greenlet with the result of awaitable that is
        # then returned to the caller (or raised as error)
>       return current.parent.switch(awaitable)  # type: ignore[no-any-return,attr-defined] # noqa: E501
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\util\_concurrency_py3k.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fn = <bound method Session.commit of <sqlalchemy.orm.session.Session object at 0x00000231829F5BE0>>
_require_await = False, args = (), kwargs = {}
context = <_AsyncIoGreenlet object at 0x0000023183139340 (otid=0x00000231CB7F74E0) dead>
switch_occurred = True
result = <coroutine object AsyncAdapt_asyncpg_connection._rollback_and_discard at 0x00000231830D6260>
value = None

    async def greenlet_spawn(
        fn: Callable[..., _T],
        *args: Any,
        _require_await: bool = False,
        **kwargs: Any,
    ) -> _T:
        """Runs a sync function ``fn`` in a new greenlet.
    
        The sync function can then use :func:`await_only` to wait for async
        functions.
    
        :param fn: The sync callable to call.
        :param \\*args: Positional arguments to pass to the ``fn`` callable.
        :param \\*\\*kwargs: Keyword arguments to pass to the ``fn`` callable.
        """
    
        result: Any
        context = _AsyncIoGreenlet(fn, getcurrent())
        # runs the function synchronously in gl greenlet. If the execution
        # is interrupted by await_only, context is not dead and result is a
        # coroutine to wait. If the context is dead the function has
        # returned, and its result can be returned.
        switch_occurred = False
        result = context.switch(*args, **kwargs)
        while not context.dead:
            switch_occurred = True
            try:
                # wait for a coroutine from await_only and then return its
                # result back to it.
>               value = await result
                        ^^^^^^^^^^^^

.venv\Lib\site-packages\sqlalchemy\util\_concurrency_py3k.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
operation = 'INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id'
parameters = (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\n...844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))

    async def _prepare_and_execute(self, operation, parameters):
        adapt_connection = self._adapt_connection
    
        async with adapt_connection._execute_mutex:
            if not adapt_connection._started:
                await adapt_connection._start_transaction()
    
            if parameters is None:
                parameters = ()
    
            try:
                prepared_stmt, attributes = await adapt_connection._prepare(
                    operation, self._invalidate_schema_cache_asof
                )
    
                if attributes:
                    self.description = [
                        (
                            attr.name,
                            attr.type.oid,
                            None,
                            None,
                            None,
                            None,
                            None,
                        )
                        for attr in attributes
                    ]
                else:
                    self.description = None
    
                if self.server_side:
                    self._cursor = await prepared_stmt.cursor(*parameters)
                    self.rowcount = -1
                else:
                    self._rows = deque(await prepared_stmt.fetch(*parameters))
                    status = prepared_stmt.get_statusmsg()
    
                    reg = re.match(
                        r"(?:SELECT|UPDATE|DELETE|INSERT \d+) (\d+)",
                        status or "",
                    )
                    if reg:
                        self.rowcount = int(reg.group(1))
                    else:
                        self.rowcount = -1
    
            except Exception as error:
>               self._handle_exception(error)

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:563: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_cursor object at 0x00000231831495A0>
error = NotNullViolationError('null value in column "content_metadata" of relation "embeddings" violates not-null constraint')

    def _handle_exception(self, error):
>       self._adapt_connection._handle_exception(error)

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <AdaptedConnection <asyncpg.connection.Connection object at 0x0000023183128B90>>
error = NotNullViolationError('null value in column "content_metadata" of relation "embeddings" violates not-null constraint')

    def _handle_exception(self, error):
        if self._connection.is_closed():
            self._transaction = None
            self._started = False
    
        if not isinstance(error, AsyncAdapt_asyncpg_dbapi.Error):
            exception_mapping = self.dbapi._asyncpg_error_translate
    
            for super_ in type(error).__mro__:
                if super_ in exception_mapping:
                    translated_error = exception_mapping[super_](
                        "%s: %s" % (type(error), error)
                    )
                    translated_error.pgcode = translated_error.sqlstate = (
                        getattr(error, "sqlstate", None)
                    )
>                   raise translated_error from error
E                   sqlalchemy.exc.IntegrityError: (sqlalchemy.dialects.postgresql.asyncpg.IntegrityError) <class 'asyncpg.exceptions.NotNullViolationError'>: null value in column "content_metadata" of relation "embeddings" violates not-null constraint
E                   DETAIL:  Failing row contains (98, 56, Journal Entry (2026-01-24T06:20:51.836019+00:00):
E                   Felt anxious d..., [-0.019126607,0.008517893,-0.047176022,0.014768274,0.008930167,0..., null, 2026-01-24 06:20:52.844605+00, 2026-01-24 06:20:52.844605+00).
E                   [SQL: INSERT INTO embeddings (user_id, content, embedding, created_at, updated_at) VALUES ($1::INTEGER, $2::VARCHAR, $3, $4::TIMESTAMP WITH TIME ZONE, $5::TIMESTAMP WITH TIME ZONE) RETURNING embeddings.id]
E                   [parameters: (56, 'Journal Entry (2026-01-24T06:20:51.836019+00:00):\nFelt anxious during solar storm\n\nMood: 4/10\nEnergy: 3/10\nTags: ', '[-0.019126607105135918,0.00851789303123951,-0.047176022082567215,0.014768273569643497,0.008930167183279991,0.027990516275167465,0.003211694536730647, ... (32392 characters truncated) ... 0.004325204994529486,0.004056490026414394,0.026827313005924225,0.029963545501232147,-0.012103211134672165,0.008054083213210106,-0.009703182615339756]', datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc), datetime.datetime(2026, 1, 24, 6, 20, 52, 844605, tzinfo=datetime.timezone.utc))]
E                   (Background on this error at: https://sqlalche.me/e/20/gkpj)

.venv\Lib\site-packages\sqlalchemy\dialects\postgresql\asyncpg.py:797: IntegrityError
------------------------------ Captured log call ------------------------------
INFO     src.app.modules.registry:registry.py:64 Registered module: astrology (layer: calculation)
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     src.app.core.memory.active_memory:active_memory.py:300 Stored master synthesis for user 56 with 3 modules
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
INFO     httpx:_client.py:1740 HTTP Request: POST https://openrouter.ai/api/v1/embeddings "HTTP/1.1 200 OK"
============================== warnings summary ===============================
src\app\modules\intelligence\synthesis\schemas.py:21
  C:\dev\GUTTERS\src\app\modules\intelligence\synthesis\schemas.py:21: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class UnifiedProfile(BaseModel):

src\app\modules\intelligence\query\schemas.py:9
  C:\dev\GUTTERS\src\app\modules\intelligence\query\schemas.py:9: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class QueryRequest(BaseModel):

src\app\modules\intelligence\hypothesis\models.py:32
  C:\dev\GUTTERS\src\app\modules\intelligence\hypothesis\models.py:32: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class Hypothesis(BaseModel):

tests/integration/test_end_to_end.py::test_complete_user_journey
  C:\dev\GUTTERS\.venv\Lib\site-packages\kerykeion\backword.py:155: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    NOW = _dt.utcnow()

tests/integration/test_end_to_end.py::test_complete_user_journey
  C:\dev\GUTTERS\.venv\Lib\site-packages\kerykeion\backword.py:188: DeprecationWarning: 'AstrologicalSubject' is deprecated and will be removed in a future major release. Please migrate to: AstrologicalSubjectFactory.from_birth_data
    _deprecated("AstrologicalSubject", "AstrologicalSubjectFactory.from_birth_data")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED tests/integration/test_end_to_end.py::test_complete_user_journey - sql...
================== 1 failed, 5 warnings in 79.49s (0:01:19) ===================
